{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f9a7c19",
   "metadata": {
    "id": "32da31fb"
   },
   "source": [
    "# Few Shot Learning- Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6cfbab",
   "metadata": {
    "id": "e6db8055"
   },
   "source": [
    "**Notebook內容：**\n",
    "1. 先備知識\n",
    "2. 模型/演算法\n",
    "3. 參考資料(連結+連結說明)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b9b170",
   "metadata": {
    "id": "d8a26917"
   },
   "source": [
    "## 1.先備知識"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42788655",
   "metadata": {
    "id": "7b01dfbf"
   },
   "source": [
    "### 1.1 Few Shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1a98a7",
   "metadata": {
    "id": "JROUouQTuHV9"
   },
   "source": [
    "* 因為target訓練資料不足，而導致模型訓練有over-fitting, under-fitting的情況\n",
    "* 訓練框架: Transfer Learning\n",
    "    * 透過在source dataset訓練而學會較好的特徵萃取，在target資料上期望能取得更好的performance\n",
    "* 訓練框架: Meta Learning\n",
    "    * 將source dataset形成多個task學習好的特徵比較或者好的更新方式等等，在target資料上期望能取得更好的performance\n",
    "* 作法: Metric based Transfer Learning/Meta Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83972a93",
   "metadata": {
    "id": "fbee2a59"
   },
   "source": [
    "### 1.2 Metric base前情提要"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6138ed3b",
   "metadata": {
    "id": "ZLFx6eCDuC3r"
   },
   "source": [
    "* 混合similarity、distance這些固定準則加入network中估計output、而非直接學習類別本身\n",
    "* 將data embed到一個高維空間中\n",
    "* 同種類相似度高(距離近)，不同種類相似度低(距離遠)\n",
    "* 比較種類參考資料(support)和待測資料(query)間的距離或相似度就能預測待測資料屬於哪個種類"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b42578d",
   "metadata": {
    "id": "85d2e2ea"
   },
   "source": [
    "## 2.模型/演算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6799f872",
   "metadata": {
    "id": "psgdpijySorn"
   },
   "source": [
    "這邊使用的model都經過兩個步驟: \n",
    "* Embedding Learning- 學習使用backbone model基本特徵萃\n",
    "* Adaptation- 讓backbone model及head model更適應於target data的訓練\n",
    "\n",
    "這邊分Head, Model , Loss三部分介紹可能增加小資料訓練強度的方案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0bb0407",
   "metadata": {
    "id": "29c45c65"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b689f53",
   "metadata": {
    "id": "9f3c5b97"
   },
   "source": [
    "### 2.1 Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8da4d0",
   "metadata": {
    "id": "d5ed4719"
   },
   "source": [
    "一些可能的building blocks，這邊是從頭train model，:\n",
    "* 有normalization, activation, pooing 的Convolution層\n",
    "* 有normalization, activation 的Dense層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14a7bc18",
   "metadata": {
    "id": "7291ce8d"
   },
   "outputs": [],
   "source": [
    "# convolution cell\n",
    "class conv(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super().__init__()\n",
    "        self.cell=nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, 3, 1, 1),\n",
    "            nn.InstanceNorm2d(ch_out),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.cell(x)\n",
    "    \n",
    "# dense cell: \n",
    "class dense(nn.Module):\n",
    "    def __init__(self,ch_in=512,ch_out=512,squeeze_input=False):\n",
    "        super().__init__()\n",
    "        self.cell=nn.Sequential(\n",
    "            nn.Linear(ch_in, ch_out),\n",
    "            nn.BatchNorm1d(ch_out),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.squeeze_input=squeeze_input\n",
    "    def forward(self,x):\n",
    "        if self.squeeze_input:\n",
    "            return self.cell(x.squeeze())\n",
    "        return self.cell(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b879d0",
   "metadata": {
    "id": "5be134bb"
   },
   "source": [
    "### 2.2 Backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf04f0d7",
   "metadata": {
    "id": "06806ab6"
   },
   "source": [
    "* 做feature extraction的部分。\n",
    "\n",
    "* 這邊簡單兜一個CNN而已，也可以使用一些pre-train backbone, 如efficient net, resnet等等\n",
    "\n",
    "* 簡單CNN結構: \n",
    "    * 數層Convolution cell\n",
    "    * 用Global average pooling平均每個feature map的local訊息\n",
    "    * 最後加一個dense cell投射到指定的latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ada775e8",
   "metadata": {
    "id": "76f8096a"
   },
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features=3,\n",
    "                 latent_features=3,\n",
    "                 hidden_featrues=(64,128,256)):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(\n",
    "            conv(3,hidden_featrues[0]),\n",
    "            *[conv(ch1, ch2) for ch1,ch2 in zip(hidden_featrues[:-1],hidden_featrues[1:])],\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            dense(hidden_featrues[-1],latent_features,squeeze_input=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.blocks(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78bb103",
   "metadata": {
    "id": "9c3c1988"
   },
   "source": [
    "### 2.3 Head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a20ffb3",
   "metadata": {
    "id": "c20d74aa"
   },
   "source": [
    "* 將latent space做切分用以分類的Output layer\n",
    "* 不同的方法在output head差很多:\n",
    "    1. Transfer Learning:\n",
    "        * 普通的dense layer output head, 將weight與前層feature相乘加上bias而已，可能會使用各種loss訓練，torch的CCE有附softmax\n",
    "        * nn.Linear\n",
    "    2. Baseline++:\n",
    "        * 每個類別有自己代表的latent vector,將latent vector與前層feature計算相似度或距離(metric)，可能會使用各種loss訓練\n",
    "        * CosineLayer(客製化) - 這邊使用cosine simmilarity做為metric，另有euclidien distance供參考\n",
    "    3. Siamese Networks:\n",
    "        * 不需要weight, 因為計算相似度或距離的來源是support照片本萃取出的latent vector\n",
    "        * 這邊不放nn.Module，在Model上定義計算方式即可\n",
    "    4. N-Way-K-Shot(以Prototypical Network為例)\n",
    "        * 不需要weight, 因為計算相似度或距離的來源是support照片本萃取出的latent vector\n",
    "        * 這邊不放nn.Module，在Model上定義計算方式即可\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38ee4c4",
   "metadata": {
    "id": "WU7a-k3ytEhN"
   },
   "source": [
    "#### 2.3.1 Cosine Similarity Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb831df",
   "metadata": {
    "id": "dleYW5NdvVwz"
   },
   "source": [
    "* 來自於Ring Loss, 就是對latent vector與output layer weight(support latent vector)算cosine simmilarity後再做Softmax\n",
    "* 與原本dense+softmax相比，若將dense的矩陣乘法改為normalized矩陣相乘則等於計算cosine similarity，其效果是可以減少數量多的類別可能形成的domination\n",
    "* 計算cosine 後再使用softmax\n",
    "\n",
    "<img src=https://i.imgur.com/WU0MXS5.png  width=\"600\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8023cd",
   "metadata": {
    "id": "un0afWIYUJU_"
   },
   "source": [
    "* Layer計算公式:$ $\n",
    "    * Dense Layer: $h=W^Tx+b$\n",
    "        * $x\\in N^{1\\times D}$是latent vector\n",
    "        * $W\\in N^{N\\times D}$為weight\n",
    "        * b是bias, \n",
    "        * $h\\in N^{1\\times N}$是logit\n",
    "    * Cosine Layer: $h=\\frac{W^Tx}{|W||x|}=cos{\\phi} $\n",
    "        * h依然是logit，但可看作x與W裡面每個latent vector夾角$\\phi$取cosine的值，也是相似度\n",
    "        * $\\phi\\in N^{1\\times N}$是夾角，相似度越大，夾角越小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00c01a0c",
   "metadata": {
    "id": "ce80211a"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# 計算cosine similarity\n",
    "def cosine(x,w):\n",
    "    return F.linear(F.normalize(x,dim=-1), F.normalize(w,dim=-1))\n",
    "# 也可以用其他metric如eucidien distance, 不過要配合不同的loss\n",
    "def euc_dist(x,w):\n",
    "    return F.pairwise_distance(x, w, 2)\n",
    "# Cosine的layer\n",
    "class MetricLayer(nn.Module):\n",
    "    def __init__(self, n_in_features,n_out_features=10,metric=cosine):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.Tensor(n_out_features, n_in_features))\n",
    "        nn.init.xavier_uniform_(self.weight,gain=1.0)\n",
    "        self.metric=metric\n",
    "    def forward(self,x):\n",
    "        return self.metric(x,self.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079ee651",
   "metadata": {
    "id": "65da6eef"
   },
   "source": [
    "### 2.4 Model\n",
    "* 組合各部件為完整分類模型\n",
    "* 不同方法在output上有差別:\n",
    "    * Transfer Learning及baseline++: \n",
    "        * 將backbone output接至head\n",
    "        * output logits在baseline++上是相似度\n",
    "    * Siamese Networks: \n",
    "        * 將兩個data丟進backbone得到latetent\n",
    "        * 將兩個latent計算相似度做為output\n",
    "        * output logits是相似度\n",
    "    * Prototypical Network: \n",
    "        * 將N*K support還有query全部丟進backbone得到latent\n",
    "        * 每個support做K shot平均成為prototype\n",
    "        * 將每個prototype與query計算相似度做為output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b24d3d63",
   "metadata": {
    "id": "069ce7f7"
   },
   "outputs": [],
   "source": [
    "WAYS=6\n",
    "SHOTS=5\n",
    "\n",
    "class ClasModel(nn.Module):\n",
    "    def __init__(self,ways,shots,backbone,head,metric=cosine):\n",
    "        super().__init__()\n",
    "        self.ways=ways\n",
    "        self.shots=shots\n",
    "\n",
    "        self.backbone=backbone\n",
    "        self.head=head\n",
    "        self.metric=metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64fa4e9",
   "metadata": {
    "id": "961436ea"
   },
   "source": [
    "#### 2.4.1 Fine-tuning(對照組, 作為baseline): (Transfer Learning)\n",
    "* Embedding Learning- \n",
    "    * 透過source dataset 訓練所有類別的分類\n",
    "    * 目標為區分source dataset所有類別\n",
    "* Adaptation- 換掉head, 再透過target dataset training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15a69b8c",
   "metadata": {
    "id": "jT2Ll9cdZaSn"
   },
   "outputs": [],
   "source": [
    "class Baseline(ClasModel):\n",
    "    def __init__(self,ways,backbone,head):\n",
    "        assert(backbone is not None)\n",
    "        super().__init__(ways,None,backbone,head)\n",
    "    def forward(self,data,label=None):\n",
    "        # Transfer Learing: backbone+ output head\n",
    "        hidden=self.backbone(data)\n",
    "        logits=self.head(hidden)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f752607",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 429,
     "status": "ok",
     "timestamp": 1648267670434,
     "user": {
      "displayName": "黃書璵",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16626369651643867989"
     },
     "user_tz": -480
    },
    "id": "babdf1c3",
    "outputId": "0653651d-838d-4178-93ec-50e7c1452e2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent shape(One batch): torch.Size([8, 2])\n",
      "Output shape: torch.Size([8, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "# Vanilla Transfer Learning\n",
    "latent_dims=2\n",
    "backbone=FeatureExtractor(latent_features=latent_dims)\n",
    "head=nn.Linear(latent_dims,WAYS)\n",
    "mode=\"transfer\"\n",
    "model=Baseline(WAYS,backbone,head)\n",
    "\n",
    "x=torch.empty((8,3,28,28),dtype=torch.float32)\n",
    "logit=model(x)\n",
    "print(\"Latent shape(One batch):\",model.backbone(x).shape)\n",
    "print(\"Output shape:\",logit.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39c454c",
   "metadata": {
    "id": "de3410cf"
   },
   "source": [
    "#### 2.4.2 Baseline++ : (Metric-based Transfer Learning)\n",
    "* Embedding Learning- \n",
    "    * 透過source dataset training,訓練所有類別的分類\n",
    "    * 目標為區分source dataset所有類別在latent space中的latent vector，各類別相應的latent vector會在過程中被訓練及儲存成weight\n",
    "* Adaptation- 換掉head(或加上更多weight), 再透過target dataset training訓練新的類別在latent space上相應的latent vector位置\n",
    "\n",
    "<img src=https://i.imgur.com/SIGtTdV.png  width=\"700\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f945c249",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1648267787561,
     "user": {
      "displayName": "黃書璵",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16626369651643867989"
     },
     "user_tz": -480
    },
    "id": "c7611dcf",
    "outputId": "40c28b32-ebbe-4795-a68e-8828163a909b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent shape(One batch): torch.Size([8, 2])\n",
      "Output shape: torch.Size([8, 6])\n"
     ]
    }
   ],
   "source": [
    "# Baseline++ with cosine metric\n",
    "\n",
    "backbone=FeatureExtractor(latent_features=latent_dims)\n",
    "head=MetricLayer(latent_dims,WAYS,cosine)\n",
    "model=Baseline(WAYS,backbone,head)\n",
    "x=torch.empty((8,3,28,28),dtype=torch.float32)\n",
    "logit=model(x)\n",
    "print(\"Latent shape(One batch):\",model.backbone(x).shape)\n",
    "print(\"Output shape:\",logit.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ea4436",
   "metadata": {
    "id": "598e62cc"
   },
   "source": [
    "#### 2.4.3 Siamese Networks: (Metric-based Meta Learning)\n",
    "* Embedding Learning-  \n",
    "    * 將source dataset中資料拆成兩兩一對的pairs training\n",
    "    * 目標是訓練萃取後特徵的對比，同class相近，異class遠離\n",
    "* Adaptation- 無，直接比較target dataset資料中support與query的部分透過backbone萃取的特徵是否足夠相近\n",
    "\n",
    "<img src=https://i.imgur.com/jK98zOa.png  width=\"600\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60f1c7fd",
   "metadata": {
    "id": "WGqAByshaCoy"
   },
   "outputs": [],
   "source": [
    "class SiameseNet(ClasModel):\n",
    "    def __init__(self,backbone,metric=cosine):\n",
    "        super().__init__(2,None,backbone,None,metric)\n",
    "    def forward(self,data,label=None):\n",
    "        # 進Embedding\n",
    "        latent=[*map(self.backbone,data.transpose(0,1))]\n",
    "        # latent算metric 這邊用cosine\n",
    "        logits=torch.stack([*map(self.metric,latent[0],latent[1])],dim=0)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5446620c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 287,
     "status": "ok",
     "timestamp": 1648267899776,
     "user": {
      "displayName": "黃書璵",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16626369651643867989"
     },
     "user_tz": -480
    },
    "id": "78094ccf",
    "outputId": "20b6965f-aaec-4783-c780-b4fbb9488fa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latents shape(One meta batch): torch.Size([2, 5])\n",
      "Output shape: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "# Siamese Networks with cosine metric\n",
    "latent_dims=5\n",
    "backbone=FeatureExtractor(latent_features=latent_dims)\n",
    "mode=\"siamese\"\n",
    "model=SiameseNet(backbone,cosine)\n",
    "x=torch.empty((8,2,3,28,28),dtype=torch.float32)\n",
    "logit=model(x)\n",
    "print(\"Latents shape(One meta batch):\",model.backbone(x[0]).shape)\n",
    "print(\"Output shape:\",logit.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f855b6",
   "metadata": {
    "id": "faeda1f3"
   },
   "source": [
    "#### 2.4.4 Prototypical Network: (Metric-based Meta Learning)\n",
    "* Embedding Learning-  將source dataset中資料拆成好幾個task\n",
    "    * 每個task有N個class分別抽K個資料\n",
    "    * 目標是訓練萃取後特徵的對比，同class相近(要是N個類別內最近的)，異class遠離\n",
    "* Adaptation- 無，直接比較target dataset資料中support與query的部分\n",
    "\n",
    "<img src=https://i.imgur.com/wLdemUU.png  width=\"600\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a1c6fc1",
   "metadata": {
    "id": "n7LZ6tTvbHW-"
   },
   "outputs": [],
   "source": [
    "class PrototypicalNet(ClasModel):\n",
    "    def __init__(self,ways,shots,backbone,metric=cosine):\n",
    "        super().__init__(ways,shots,backbone,None,metric)\n",
    "    def meta_forward(self,dataset,label=None):\n",
    "        # 全部進embedding\n",
    "        latent=torch.stack([*map(self.backbone,dataset)],dim=0)\n",
    "        latent_q,latent_s=latent[:,self.ways*self.shots:],latent[:,:self.ways*self.shots]\n",
    "        # 計算 prototypes\n",
    "        latent_proto=torch.stack([torch.mean(l,dim=1) for l in torch.split(latent_s,self.shots,dim=1)],dim=1)\n",
    "        logits=torch.cat([*map(self.metric,latent_q,latent_proto)],dim=0)\n",
    "        return logits\n",
    "    def save_prototypes(self,dataset):\n",
    "        with torch.no_grad():\n",
    "            latent_s=self.backbone(dataset)\n",
    "            self.latent_proto=torch.stack([torch.mean(l,dim=0) for l in torch.split(latent_s,self.shots,dim=0)],dim=0).detach()\n",
    "    def forward(self,data,label=None):\n",
    "        latent_q=self.backbone(data)[:,np.newaxis,...]\n",
    "        logits=torch.cat([self.metric(self.latent_proto,qq).transpose(0,1) for qq in latent_q],dim=0)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "682beef1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 837,
     "status": "ok",
     "timestamp": 1648268184343,
     "user": {
      "displayName": "黃書璵",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16626369651643867989"
     },
     "user_tz": -480
    },
    "id": "ddee2aee",
    "outputId": "73adbd2a-2672-428a-d0c3-3556d384a435"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latents shape(One meta batch): torch.Size([31, 5])\n",
      "Output shape: torch.Size([8, 6])\n"
     ]
    }
   ],
   "source": [
    "# Prototypical Network with cosine metric\n",
    "\n",
    "mode=\"proto\"\n",
    "model=PrototypicalNet(WAYS,SHOTS,backbone,cosine)\n",
    "x=torch.empty((8,WAYS*SHOTS+1,3,28,28),dtype=torch.float32)\n",
    "logit=model.meta_forward(x)\n",
    "print(\"Latents shape(One meta batch):\",model.backbone(x[0]).shape)\n",
    "print(\"Output shape:\",logit.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a1e1b",
   "metadata": {
    "id": "ba59b1c2"
   },
   "source": [
    "### 2.5 Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da088866",
   "metadata": {
    "id": "903096e9"
   },
   "source": [
    "* 定義計算output logits與label的誤差的方式。\n",
    "* 這邊提供幾種loss:\n",
    "    * Categorical Cross Entropy(CCE) -> (torch原生)可以給transfer learning, baseline++, prototypical network用\n",
    "    * Focal Loss -> 加強正確類別的權重，可以給transfer learning, baseline++, prototypical network用\n",
    "    * Contrastive Loss -> 可以給Siamese Network用，但這邊是給euclidien distance metric用的，若是cosine similarity則可直接用BCE\n",
    "    * Add Margin Loss -> 人臉辨識家族CosineFace用的loss，拉開不同class的cosine值，給cosine metric用: baseline++, prototypical network\n",
    "    * Arc Margin Loss -> 人臉辨識家族ArcFace用的loss，拉開不同class的徑度值(cosine裡面的那個角度)，給cosine metric用: baseline++, prototypical network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a3075b",
   "metadata": {
    "id": "hz45wupAN3wU"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81fea9a5",
   "metadata": {
    "id": "RSbSosGPBVAq"
   },
   "source": [
    "### 2.5.1 Special Loss for Transfer Learning\n",
    "* Focal Loss: 降低easy case中loss gradient,增加hard case中loss gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99860c60",
   "metadata": {
    "id": "cfiPnBBqN7Pw"
   },
   "source": [
    "<img src=https://i.imgur.com/u9OUrX2.png  width=\"400\" height=\"250\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25ad9d3b",
   "metadata": {
    "id": "mX2dJy6aBTQx"
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, eps=1e-10):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.eps = torch.tensor(eps,dtype=torch.float32)\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "    def forward(self,  y_pred,y_true):\n",
    "        # 計算cross entropy\n",
    "        logp = self.ce(y_pred+self.eps, y_true)\n",
    "        # 計算乘上gamma次方後的entropy反方機率(將對比放大)\n",
    "        p = torch.exp(-logp)\n",
    "        loss = (1 - p) ** self.gamma * logp\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fc3379",
   "metadata": {
    "id": "kee_b_-5__Db"
   },
   "source": [
    "### 2.5.2 Loss for Siamese Netwrok\n",
    "* Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b71bb519",
   "metadata": {
    "id": "s2Gx0QQk__SC"
   },
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self,m=1):\n",
    "        super().__init__()\n",
    "        self.m=m\n",
    "        self.activation=torch.sigmoid\n",
    "        self.loss_fn=nn.BCELoss()\n",
    "        self.z=torch.tensor(0.,dtype=torch.float32,requires_grad=False)\n",
    "    def forward(self, y_pred,y_true):\n",
    "        # 兩者同組時，算square\n",
    "        # 兩者不同組時，算margin- distance值，若distance大於margin則不用再拉伸兩者distance\n",
    "        loss=torch.mean(y_true * torch.square(y_pred)+ \n",
    "                        (1 - y_true)* torch.square(torch.maximum(self.m - y_pred, self.z)\n",
    "                        ),dim=-1,keepdims=True)\n",
    "        return loss.mean()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1485e8f",
   "metadata": {
    "id": "E_Aizthes4sL"
   },
   "source": [
    "### 2.5.3 Loss for Cosine logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60875609",
   "metadata": {
    "id": "qd_MiLrGuVuI"
   },
   "source": [
    "前述Baseline++是使用Metric based的方法來對待測image的latent vector與各類別support訓練出來的latent vector算cosine simillarity，並且在target task時可以訓練target類別的support latent vector。\n",
    "\n",
    "那這個cosine similarity 也可以有更多種方式來緩和資料不平衡的影響及增強類別的區辨。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c4d658",
   "metadata": {
    "id": "ojSHRGNvv9HV"
   },
   "source": [
    "*  CosineMargin Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d79e4a",
   "metadata": {
    "id": "WY2_kDoYwHHd"
   },
   "source": [
    "* 出自CosineFace, 先計算cosine simmilarity\n",
    "* 在正確類別cosine值扣掉一個margin，使得訓練變得更困難，也使得每個類別的區分更明顯\n",
    "(增加Way數大的時候的準確度)\n",
    "\n",
    "<img src=https://i.imgur.com/75WvROU.png  width=\"600\" height=\"300\">\n",
    "\n",
    "(借用L-Softmax的圖形)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a737c22c",
   "metadata": {
    "id": "UhOEaEtgAVzb"
   },
   "outputs": [],
   "source": [
    "class AddMarginLoss(nn.Module):\n",
    "    def __init__(self, s=15.0, m=0.40,loss_fn=FocalLoss()):\n",
    "        super().__init__()\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.loss_fn=loss_fn\n",
    "    def forward(self, cosine, label=None):\n",
    "        # 扣掉對cosine的margin\n",
    "        cos_phi = cosine - self.m\n",
    "        # 將onehot沒選中的類別不套用margin，onehot選中的套用margin     \n",
    "        one_hot=F.one_hot(label, num_classes=- 1).to(torch.float32)\n",
    "        metric = (one_hot * cos_phi) + ((1.0 - one_hot) * cosine)\n",
    "        # 將輸出對比放大\n",
    "        metric *= self.s\n",
    "        return self.loss_fn(metric,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea30c5",
   "metadata": {
    "id": "69rzESoFyUq8"
   },
   "source": [
    "* ArcMargin Loss\n",
    "    * 出自ArcFace, 要先計算cosine simmilarity\n",
    "    * 將正確類別cosine值轉成arc(角度)再加上一個margin，使得訓練變得更困難，也使得每個類別的區分更明顯\n",
    "    * 另外也有其他使用margin的loss，都是在增加不同類別latent vecor間的角度，以下為列表\n",
    "    <img src=https://i.imgur.com/SHyaRnc.png  width=\"400\" height=\"150\">\n",
    "    * 使用margin對角度的影響如示意圖\n",
    "    <img src=https://i.imgur.com/h1dFJ8z.png  width=\"500\" height=\"150\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a803cc8f",
   "metadata": {
    "id": "35090ea6"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class ArcMarginLoss(AddMarginLoss):\n",
    "    def __init__(self, s=32.0, m=0.40, easy_margin=False,loss_fn=FocalLoss()):\n",
    "        super().__init__(s,m,loss_fn)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "\n",
    "        # make the function cos(theta+m) monotonic decreasing while theta in [0°,180°]\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "        self.eps = 1e-6\n",
    "    def forward(self, cosine, label=None):\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2) + self.eps)\n",
    "        # cos(phi)cos(m)-sin(phi)sin(m)變成cos(phi + m)\n",
    "        # 這個margin加上去使得角度phi需要更小才能使指定類別在softmax(cos(phi))時最大\n",
    "        cos_phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            # cosine如果不夠大就不用不套用phi margin\n",
    "            cos_phi = torch.where(cosine > 0, cos_phi, cosine)\n",
    "        else:\n",
    "            # 更加嚴格，若cosine(phi)大於margin則套用phi margin規則\n",
    "            #          若cosine(phi)小於margin則套用cosine margin規則\n",
    "            cos_phi = torch.where(cosine > self.th, cos_phi, cosine - self.mm)\n",
    "            \n",
    "        # 將onehot沒選中的類別不套用margin，onehot選中的套用margin    \n",
    "        one_hot=F.one_hot(label, num_classes=- 1).to(torch.float32)\n",
    "        metric = (one_hot * cos_phi) + ((1.0 - one_hot) * cosine)\n",
    "        # 將輸出對比放大\n",
    "        metric *= self.s\n",
    "        return self.loss_fn(metric,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c16e975",
   "metadata": {
    "id": "39712ff2"
   },
   "source": [
    "試看loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8ef02ca",
   "metadata": {
    "id": "1c24f3f1"
   },
   "outputs": [],
   "source": [
    "cce=nn.CrossEntropyLoss()\n",
    "focal_cce=FocalLoss(2,SHOTS)\n",
    "contrastive=ContrastiveLoss(m=1)\n",
    "addmarginloss=AddMarginLoss(s=2.0, m=0.40)\n",
    "arcmarginloss=ArcMarginLoss(s=2.0, m=0.40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8baabe9",
   "metadata": {
    "id": "1c504eed"
   },
   "outputs": [],
   "source": [
    "yy=torch.tensor([0,1,2,3])\n",
    "y_gt=F.one_hot(yy, num_classes=- 1).to(torch.float32)\n",
    "y_best=y_gt-0.5+torch.randn((4,4),dtype=torch.float32)/100\n",
    "y_good=y_gt-0.5+torch.randn((4,4),dtype=torch.float32)/10\n",
    "y_noisy=y_gt-0.5+torch.randn((4,4),dtype=torch.float32)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f802f4a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 384,
     "status": "ok",
     "timestamp": 1648268745587,
     "user": {
      "displayName": "黃書璵",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16626369651643867989"
     },
     "user_tz": -480
    },
    "id": "IN-bPLJvXc-D",
    "outputId": "d2359bd0-c3ef-428e-8c49-0042bda74733"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2338)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrastive(y_gt,y_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc00fa02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "executionInfo": {
     "elapsed": 350,
     "status": "ok",
     "timestamp": 1648268746261,
     "user": {
      "displayName": "黃書璵",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16626369651643867989"
     },
     "user_tz": -480
    },
    "id": "82a71bfc",
    "outputId": "05f2d2cb-d85c-45b1-fc4c-cfb6bde7b350"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbfa0054650>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.subplot(1,4,1);plt.imshow(y_gt)\n",
    "plt.subplot(1,4,2);plt.imshow(y_best)\n",
    "plt.subplot(1,4,3);plt.imshow(y_good)\n",
    "plt.subplot(1,4,4);plt.imshow(y_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d65f62ad",
   "metadata": {
    "id": "5e9602ca"
   },
   "outputs": [],
   "source": [
    "loss_cce=[cce(pred,yy).item() for pred in [y_gt,y_best,y_good,y_noisy]]\n",
    "loss_focal=[focal_cce(pred,yy).item() for pred in [y_gt,y_best,y_good,y_noisy]]\n",
    "loss_add=[addmarginloss(pred,yy).item() for pred in [y_gt,y_best,y_good,y_noisy]]\n",
    "loss_arc=[arcmarginloss(pred,yy).item() for pred in [y_gt,y_best,y_good,y_noisy]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7f67527",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1648268781216,
     "user": {
      "displayName": "黃書璵",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16626369651643867989"
     },
     "user_tz": -480
    },
    "id": "5ef48d2a",
    "outputId": "a4c752b2-188d-4e2e-f12d-b8c96c05d861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best-------------good-------------noisy\n",
      "CCE: [0.7436683773994446, 0.7427719831466675, 0.7088544964790344, 0.8183890581130981]\n",
      "Focal: [0.2046871930360794, 0.2041083127260208, 0.1827802062034607, 0.2556013762950897]\n",
      "Addmargin: [0.1450444757938385, 0.1441778689622879, 0.11511790007352829, 0.2652021646499634]\n",
      "Arcmargin: [0.04045689105987549, 0.13206274807453156, 0.10244133323431015, 0.22916999459266663]\n"
     ]
    }
   ],
   "source": [
    "print(\"best-------------good-------------noisy\")\n",
    "print(\"CCE:\",loss_cce)\n",
    "print(\"Focal:\",loss_focal)\n",
    "print(\"Addmargin:\",loss_add)\n",
    "print(\"Arcmargin:\",loss_arc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7037ae3e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1648268817078,
     "user": {
      "displayName": "黃書璵",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16626369651643867989"
     },
     "user_tz": -480
    },
    "id": "1b455b9f",
    "outputId": "4df3f3ab-0b75-4820-bf1d-5dc67d2b2002"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'normalized loss')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def div_first(x):\n",
    "    return [*map(lambda h: h/x[0],x)]\n",
    "labels=[\"golden\",\"best\",\"good\",\"noisy\"]\n",
    "plt.bar(labels,div_first(loss_arc),alpha=0.8)\n",
    "plt.bar(labels,div_first(loss_add),alpha=0.8)\n",
    "plt.bar(labels,div_first(loss_focal),alpha=0.8)\n",
    "plt.bar(labels,div_first(loss_cce),alpha=0.8)\n",
    "plt.legend([\"arcmargin\",\"addmargin\",\"focal\",\"cce\"])\n",
    "plt.ylabel(\"normalized loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fb7d58",
   "metadata": {
    "id": "255aa34a"
   },
   "source": [
    "基本arcmargin與addmargin可以讓loss數值對錯誤答案敏感化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1dc561",
   "metadata": {
    "id": "72ox649ear5b"
   },
   "source": [
    "## 3.參考資料\n",
    "* [Chen, W. Y., Liu, Y. C., Kira, Z., Wang, Y. C. F., & Huang, J. B. (2019). A closer look at few-shot classification. arXiv preprint arXiv:1904.04232.](https://arxiv.org/abs/1904.04232)\n",
    "    - Baseline++ 原文(台大王鈺強老師團隊)\n",
    "* [Bromley, J., Guyon, I., LeCun, Y., Säckinger, E., & Shah, R. (1993). Signature verification using a\" siamese\" time delay neural network. Advances in neural information processing systems, 6.](https://proceedings.neurips.cc/paper/1993/hash/288cc0ff022877bd3df94bc9360b9c5d-Abstract.html)\n",
    "    - Siamese Networks原文\n",
    "* [Snell, J., Swersky, K., & Zemel, R. S. (2017). Prototypical networks for few-shot learning. arXiv preprint arXiv:1703.05175.](https://arxiv.org/abs/1703.05175)\n",
    "    - Prototypical networks原文\n",
    "* [Lin, T. Y., Goyal, P., Girshick, R., He, K., & Dollár, P. (2017). Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision (pp. 2980-2988). ](https://arxiv.org/abs/1708.02002v2)\n",
    "    - Focal Loss原文\n",
    "* [Wang, H., Wang, Y., Zhou, Z., Ji, X., Gong, D., Zhou, J., ... & Liu, W. (2018). Cosface: Large margin cosine loss for deep face recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 5265-5274).](https://arxiv.org/abs/1801.09414)\n",
    "    - CosineFace原文\n",
    "* [Deng, J., Guo, J., Xue, N., & Zafeiriou, S. (2019). Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 4690-4699).](https://arxiv.org/abs/1801.07698)\n",
    "    - ArcFace原文\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddaf779",
   "metadata": {
    "id": "e685e4cf"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2_Few Shot Learning- 1 Models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
